---
title: "DATA607_Project4_Chunjie_Nan"
author: "Chunjie Nan"
date: "11/13/2021"
output:
  html_document:
    code_download: yes
    code_folding: hide
    highlight: pygments
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```




## Task
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/ 


## Loading Library
```{r}
library(tm)
library(knitr)
library(plyr)
library(wordcloud)
library(tidyverse)
library(tm)
library(magrittr)
library(data.table)
library(e1071)
library(caret)
library(randomForest)
```


## Loading Data

[data link](https://spamassassin.apache.org/old/publiccorpus/)

```{r}
spam<- '/Users/chunjienan/Desktop/MS Data Science/DATA 607 DADA ACQUISITION MANAGEMENT/2021 fall/Week 12. Nov 8 - Nov 14/Project4/spamdata/spam'
count_spam<-length(list.files(path = spam))
count_spam
ham<-'/Users/chunjienan/Desktop/MS Data Science/DATA 607 DADA ACQUISITION MANAGEMENT/2021 fall/Week 12. Nov 8 - Nov 14/Project4/spamdata/easy_ham'
count_ham<-length(list.files(path = ham))
count_ham

spam_list<-list.files(spam)
ham_list<-list.files(ham)
```


There are 501 Spam and 2501 Ham in the data set. 


## Data Cleaning


```{r}
ham_doc <- NA
for(i in 1:length(ham_list))
{
  path<-paste0(ham, "/", ham_list[1])  
  text <-readLines(path)
  list<- list(paste(text, collapse="\n"))
  ham_doc = c(ham_doc,list)
  
}

spam_doc <- NA
for(i in 1:length(spam_list))
{
  path<-paste0(spam, "/", spam_list[1])  
  text <-readLines(path)
  list<- list(paste(text, collapse="\n"))
  spam_doc = c(spam_doc,list)
}


ham_data_frame <-as.data.frame(unlist(ham_doc),stringsAsFactors = FALSE)
ham_data_frame$type <- "ham"
colnames(ham_data_frame) <- c("text","type")

spam_data_frame <-as.data.frame(unlist(spam_doc),stringsAsFactors = FALSE)
spam_data_frame$type <- "spam"
colnames(spam_data_frame) <- c("text","type")

# Bind the data frames
spam_ham_data <- rbind(ham_data_frame, spam_data_frame)
nrow(spam_ham_data)
table(spam_ham_data$type)
```

There are total of 3004 of observations is in the data set, and 2052 are ham and 502 are spam in this combined data set.


## Create Corpuses from Email

```{r}
corpus <- VCorpus(VectorSource(spam_ham_data$text))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
```


## Word Cloud and Matrix

```{r}
dtm<- DocumentTermMatrix(corpus)
Spam<-which(spam_ham_data$type == 'spam')
Ham<-which(spam_ham_data$type == 'ham')
wordcloud(corpus[Spam], max.words = 100, random.order = FALSE, colors=c('Red'))
wordcloud(corpus[Ham],max.words = 100 ,random.order = FALSE, colors=c('Blue'))



```


## Split Data and Corpus for Traing Data and Test Data

```{r}
spam_ham_data$text[spam_ham_data$text==""] <- "NA"
split_index <- createDataPartition(spam_ham_data$type, p=0.70, list=FALSE)
train <- spam_ham_data[split_index,]
test <- spam_ham_data[-split_index,]


# Corpus for training data and test data
train_corpus <- Corpus(VectorSource(train$text))
train_corp <- tm_map(train_corpus ,removeNumbers)
train_corp <- tm_map(train_corp,removePunctuation)
train_corp<- tm_map(train_corp, removeWords,stopwords())
train_corp<- tm_map(train_corp,stripWhitespace)


test_corpus <- Corpus(VectorSource(test$text))
test_corp<- tm_map(test_corpus,removeNumbers)
test_corp <- tm_map(test_corp,removePunctuation)
test_corp  <- tm_map(test_corp,removeWords, stopwords())
test_corp<- tm_map(test_corp,stripWhitespace)


train_dtm <- DocumentTermMatrix(train_corp)
test_dtm <- DocumentTermMatrix(test_corp)


convert <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

sms.train <- apply(train_dtm, 2, convert)
sms.test <- apply(test_dtm, 2, convert)
```


## Modeling and Prediction

### Naive Bayes Classification
```{r}
# classification of email
naive_classifier <- naiveBayes(sms.train, factor(train$type))
pred_naive <- predict(naive_classifier, newdata=sms.test)
naive_matrix<-table(pred_naive, test$type)
naive_matrix
```


## Accuracy of Naive Classification 

Accuracy = (TP + TN)/Total
```{r}
TP<- 750 + 150
TN<- 0 + 0
Accuracy_rate<-(TP + TN)/sum(naive_matrix)*100

confusionMatrix(pred_naive, as.factor(test$type), positive = 'spam', dnn = c('pred','actual'))

print(paste0('The accuracy_rate of naive classification model is ' , Accuracy_rate,'%'))

```


## Conclusion

Naive Bayes Classification is considered one of the best options to do classification analysis. It allowed me to get the 100% of 
accuracy of the spam email detection. According to the confusion matrix function, the model classified 100 % of spams and hams correctly.

